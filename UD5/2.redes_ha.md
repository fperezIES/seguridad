


# Alta disponibilidad en redes

El diseño de una LAN resiliente implica incorporar redundancias y mecanismos de failover a distintos niveles de la red, combinando tecnologías y protocolos que garanticen una alta disponibilidad. Esto incluye la implementación de topologías con enlaces múltiples y agregación de enlaces, el uso de protocolos de enrutamiento redundante (como VRRP o HSRP) y la aplicación de STP (Spanning Tree Protocol) o sus variantes optimizadas (RSTP, MSTP) para prevenir bucles sin comprometer la continuidad del servicio. . El objetivo es mantener la disponibilidad de la red incluso frente a incidencias y garantizar la operatividad in entornos empresariales donde la conectividad constante es esencial.

## Enlaces múltiples y agregación

Las topologías con enlaces múltiples y la agregación de enlaces permiten obtener mayor capacidad de transmisión, flexibilidad y resiliencia en la red. Al disponer de varios enlaces físicos entre dispositivos de red (por ejemplo, entre switches o entre un servidor y un switch), el uso de protocolos como LACP (802.3ad) posibilita la unión lógica de estas interfaces para formar un enlace virtual más robusto y con mayor ancho de banda. Este enfoque no solo incrementa la capacidad total, sino que también garantiza la continuidad del servicio frente a fallos de un enlace individual, ya que el resto permanecerá operativo. Estas configuraciones mejoran la eficiencia y aseguran un suministro más consistente, reduciendo la latencia y el riesgo de interrupciones, especialmente en entornos críticos y con altos volúmenes de tráfico.

### Topologías resilientes

Entre las topologías más comunes para construir redes resilientes destacan aquellas que incorporan enlaces redundantes y rutas alternativas entre sus nodos. 

* Topologías en **malla**, donde cada dispositivo está conectado con múltiples otros, permiten mantener la conectividad incluso si falla uno de los enlaces. 
* Topologías en **anillo** pueden implementar protocolos de recuperación rápida que, ante un corte en un tramo, desvían el tráfico por el camino alternativo disponible. 
* Topologías **jerárquicas**, con niveles de núcleo, distribución y acceso, también facilitan la resiliencia: incorporando múltiples rutas hacia el núcleo de la red y aplicando protocolos de agregación de enlaces y enrutamiento redundante se puede evitar que la caída de un nodo específico afecte significativamente al resto de la infraestructura. 
	* La capa de **núcleo** (core) proporciona la “espina dorsal” de la red, ofreciendo conmutación y encaminamiento de alta velocidad para interconectar las distintas partes de la infraestructura sin aplicar políticas complejas. 
	* La capa de **distribución** actúa como punto intermedio, donde se aplican políticas de seguridad, filtrado, agregación de rutas y balanceo de carga, consolidando el tráfico que proviene de la capa de acceso. 
	* Finalmente, la capa de **acceso** es la más cercana al usuario, conectando equipos finales (PCs, impresoras, teléfonos IP) a la red y aplicando las configuraciones básicas (VLAN, QoS) para asegurar una conectividad adecuada. 

En conjunto, estas configuraciones favorecen la alta disponibilidad, ya que ningún único punto de la red se convierte en un cuello de botella ni en un punto único de fallo. 


### Agregación de enlaces

La agregación de enlaces, también conocida como Link Aggregation, permite combinar múltiples interfaces físicas en un único canal lógico con mayor ancho de banda y resiliencia. Entre los protocolos más utilizados para lograrlo se encuentran **LACP** (Link Aggregation Control Protocol), definido en el estándar IEEE 802.3ad, y otros métodos propietarios o estáticos que ofrecen capacidades similares de redundancia y equilibrio de carga a nivel de enlace.

**LACP** (Link Aggregation Control Protocol) se emplea para unir varias interfaces de red físicas en un único canal lógico entre un servidor y uno o varios switches. Este enfoque se traduce en una mayor disponibilidad (HA, High Availability) y un mayor ancho de banda agregado. Al configurar LACP en el servidor, se crea un bond o team de interfaces que, a ojos del sistema operativo, actúa como una sola conexión, mientras que el switch remoto también se configura para reconocer y gestionar este grupo de puertos. De esta manera, si uno de los enlaces físicos falla, el resto sigue activo, manteniendo la conexión sin interrupciones. Además, el tráfico puede distribuirse entre todas las interfaces, maximizando el rendimiento y la resiliencia de la conexión en entornos críticos.

**EtherChannel** es una tecnología desarrollada originalmente por Cisco que permite agrupar varias conexiones Ethernet físicas en un solo enlace lógico. Este “canal” unificado ofrece un ancho de banda sumado de todas las interfaces participantes, al tiempo que incrementa la redundancia: si falla una de las líneas, las restantes siguen operativas, evitando interrupciones en el servicio. EtherChannel, que puede configurarse de forma estática o empleando protocolos como PAgP (Port Aggregation Protocol) o LACP, se utiliza comúnmente entre switches o entre switches y servidores, optimizando la eficiencia, disponibilidad y rendimiento en redes de nivel empresarial.



## VRRP

VRRP (Virtual Router Redundancy Protocol) es un protocolo diseñado para incrementar la disponibilidad de los servicios de red al permitir la configuración de un “router virtual” compuesto por varios enrutadores físicos. El objetivo principal de VRRP es asegurar que, en caso de que el router principal (el que soporta el tráfico hacia el exterior) falle, otro router de respaldo pueda tomar automáticamente su lugar sin que los usuarios perciban una caída en la conectividad.

### Bases del funcionamiento

1. **Router Virtual y Dirección IP Virtual**:  
    Con VRRP, se crea una dirección IP virtual compartida entre un conjunto de routers físicos. De estos routers, uno se elige como “Master” (o activo) y el resto como “Backups”. El “Master” es el que responde realmente a las peticiones ARP y enrutará el tráfico de salida de la red interna hacia el exterior. Si el Master falla, uno de los routers Backup detectará la falta de anuncios VRRP y tomará el rol Master, asumiendo la dirección IP virtual y continuando el enrutamiento.
    
2. **Elección del Maestro**:  
    El router que tiene mayor prioridad (un valor configurado en el protocolo, por defecto 100, pero configurable entre 1 y 254) es el que se convierte en el Master. Si hay un empate, la dirección IP más alta puede servir como desempate, o se puede especificar manualmente.
    
3. **Publicidad Multicast**:  
    VRRP utiliza mensajes multicast (generalmente a la dirección 224.0.0.18 con protocolo IP 112) para anunciar el estado del Master y permitir que los Backup estén al tanto de la disponibilidad del mismo. Estos anuncios se envían periódicamente, y si los Backup dejan de recibirlos, asumen que el Master ha fallado.
    
4. **Transparencia para el usuario final**:  
    Desde la perspectiva de las estaciones de trabajo, solo ven una puerta de enlace (gateway) con una dirección IP estática. No es necesario reconfigurar los clientes, ya que el protocolo se encarga de mantener esa dirección accesible.
    

### Software para ponerlo en práctica

- **[Keepalived](https://www.keepalived.org/) (Linux)**:  
    Keepalived es una herramienta muy utilizada en entornos Linux para implementar VRRP. Permite la configuración de routers virtuales, gestión de prioridades y tiempos de anuncio, así como la monitorización de la disponibilidad de servicios. Con Keepalived podemos, por ejemplo, configurar dos servidores Linux con una IP virtual compartida que actúen como gateway de salida para una red interna.
    
- **[VRRPd (Linux)](https://gitlab.com/fredbcode/Vrrpd)**:  
    VRRPd es un demonio específico para Linux que implementa VRRP. Su configuración es relativamente sencilla y se integra bien con otros servicios. Aunque Keepalived es más popular por sus opciones adicionales, VRRPd es una alternativa más ligera.
    
- **Equipos de red con VRRP nativo**:  
    Muchos switch L3 y routers profesionales, como los de **Cisco** (en su variante propietaria HSRP, o VRRP estándar), **Juniper**, HP, **Aruba** o **Dell**, soportan VRRP de forma nativa. 
   

<!--
En entornos de prácticas, si se cuenta con material físico, se puede configurar directamente en estos dispositivos.    

En entornos de laboratorio virtualizados, es posible montar máquinas virtuales (con VirtualBox, VMware o KVM) con Linux y configurar VRRP con Keepalived, creando escenarios de tolerancia a fallos y simulando la pérdida del router principal para comprobar la conmutación transparente a un router de respaldo. Esto es especialmente útil para estudiantes de FP, ya que permite experimentar en un entorno controlado con una mínima inversión en infraestructura.
-->


# Distribución de carga

## Balanceador de carga

Un **balanceador de carga** es un sistema (software o hardware) que distribuye el tráfico de red entrante entre múltiples servidores backend. Su objetivo principal es mejorar la **disponibilidad**, **rendimiento** y **escalabilidad** de una aplicación o servicio al evitar que un único servidor se sature.

### **Funciones principales:**

1. **Distribución del tráfico:** Usa algoritmos como **round-robin**, **least connections** o **IP hash** para repartir las solicitudes de manera eficiente.
	- **Round-robin**: Este algoritmo distribuye las peticiones secuencialmente entre los servidores disponibles. Es sencillo de implementar, pero no considera factores como la carga real en cada servidor.    
	- **Least connections**: Este algoritmo envía la siguiente petición al servidor que tenga en ese momento el menor número de conexiones activas. De esta forma, se busca equilibrar la carga de forma dinámica, adaptándose a las variaciones en el tiempo que cada servidor tarda en atender las solicitudes, lo que da como resultado un reparto más justo y eficiente.    
	- **IP hash**: Con este método, el balanceador asigna cada cliente a un servidor específico calculando una función hash basada en la dirección IP del cliente. De este modo, un mismo cliente tiende a ser redirigido siempre al mismo servidor, lo que facilita la persistencia de sesión y puede resultar conveniente para ciertas aplicaciones que almacenan información específica del usuario en memoria. Sin embargo, es menos flexible a la hora de equilibrar la carga, ya que la elección está condicionada por la IP del cliente.	
2. **Alta disponibilidad:** Detecta si un servidor está inactivo y redirige el tráfico a los servidores que están operativos.
3. **Escalabilidad:** Permite agregar o quitar servidores backend según las necesidades de la aplicación.
4. **Seguridad:** Puede actuar como un punto de control para proteger los servidores backend del acceso directo.

### **Usos comunes:**

- Balancear solicitudes web (HTTP/HTTPS).
- Distribuir tráfico en bases de datos, aplicaciones o servicios.
- Soportar sistemas redundantes para garantizar disponibilidad.

### Ejemplos de balanceadores de carga: 

- [**HAProxy**](https://www.haproxy.org/): HAProxy es una solución de código abierto especializada en balanceo de carga y proxy inverso para aplicaciones. Destaca por su alta eficiencia, estabilidad y capacidad para gestionar un gran volumen de peticiones simultáneas. 
- [**Nginx**](https://nginx.org/en/docs/http/load_balancing.html): Nginx es un servidor web de alto rendimiento que destaca por su eficiencia en la gestión de conexiones simultáneas y su bajo consumo de recursos. Además de servir contenido estático, puede actuar como un potente balanceador de carga, distribuyendo el tráfico entre varios servidores de aplicaciones.
- [**AWS ELB**](https://aws.amazon.com/es/elasticloadbalancing/): Elastic Load Balancing (ELB) distribuye automáticamente el tráfico de aplicaciones entrantes entre varios destinos y dispositivos virtuales en una o varias zonas de disponibilidad (AZ).
- [**F5 Big-IP**](https://www.f5.com/es_es/products/big-ip): F5 BIG-IP es una familia de dispositivos y soluciones de hardware y software que funcionan como controladores de entrega de aplicaciones (ADCs). Ofrece funcionalidades avanzadas de balaceador de carga, seguridad, aceleración del tráfico y optimización del rendimiento.

## Proxy inverso

Un **proxy inverso** es un servicio intermedio situado entre el cliente externo (por ejemplo, un navegador web) y uno o varios servidores internos que atienden la petición real. Mientras un proxy tradicional suele usarse para que el cliente acceda a internet a través de él, el proxy inverso actúa representando al servidor o servidores internos de cara a los clientes. En otras palabras, cuando un usuario solicita acceder a un sitio web con un dominio específico, el proxy inverso recibe la solicitud y luego la reenvía al servidor adecuado dentro de la red interna, devolviendo finalmente la respuesta al cliente como si fuera el propio servidor.

**Funciones principales de un proxy inverso:**

1. **Distribución de carga:** Puede dirigir el tráfico entrante entre varios servidores, de forma que ninguno quede saturado. Así se aprovechan mejor los recursos disponibles.
    
2. **Cacheo de contenido:** Puede almacenar en caché respuestas estáticas (imágenes, hojas de estilo, scripts), reduciendo la carga en el servidor de origen y agilizando la entrega al cliente.
    
3. **Seguridad y filtrado:** El proxy inverso actúa como una barrera, ocultando la estructura interna de la red y permitiendo aplicar políticas de seguridad, filtrado de peticiones maliciosas, e incluso detección de intrusiones.
    
4. **Terminación de SSL/TLS:** Permite gestionar el cifrado directamente en el proxy inverso, descargando al servidor de la necesidad de desencriptar el tráfico, lo cual se traduce en una menor carga de trabajo.


# Bibliografía

- Documentación LACP: [IEEE 802.3ad](https://ieeexplore.ieee.org/document/810387). 
- Guía de Keepalived: [keepalived.org](https://www.keepalived.org/)